{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key='',  # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "part=1\n",
    "dataset='Grocery_and_Gourmet_Food'\n",
    "phase='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test.jsonl 是一个本地示例文件，purpose必须是batch\n",
    "file_object = client.files.create(file=Path(f\"{dataset}_random_{phase}_part{part}.jsonl\"), purpose=\"batch\")\n",
    "\n",
    "print(file_object.model_dump_json())\n",
    "print(file_object.id)  # 打印文件id\n",
    "file_object_id=file_object.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = client.batches.create(\n",
    "    input_file_id=file_object_id,  # 上传文件返回的 id\n",
    "    endpoint=\"/v1/chat/completions\",  # 大语言模型固定填写，/v1/chat/completions\n",
    "    completion_window=\"24h\"  # 当前只支持24h，24小时未运行完会超时\n",
    ")\n",
    "print(batch)\n",
    "print(batch.id)  # 打印Batch任务的id\n",
    "batch_id=batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查询\n",
    "batch = client.batches.retrieve('')  # 将batch_id替换为Batch任务的id\n",
    "print(batch)\n",
    "print(batch.error_file_id)\n",
    "print(batch.output_file_id)  # 打印输出文件id\n",
    "error_file_id=batch.error_file_id\n",
    "output_file_id=batch.output_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = client.files.content(file_id=output_file_id)\n",
    "# 打印结果文件内容\n",
    "print(content.text)\n",
    "# 保存结果文件至本地\n",
    "content.write_to_file(f\"{dataset}_random_{phase}_part{part}_result.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_json_content(content):\n",
    "    \"\"\"\n",
    "    Clean the JSON-like content by removing trailing commas in all nested structures.\n",
    "    \"\"\"\n",
    "    # Remove trailing commas in dictionaries or lists\n",
    "    cleaned_content = re.sub(r',\\s*$', '', content.strip())\n",
    "    return cleaned_content\n",
    "\n",
    "def process_jsonl_file_with_trailing_comma_fix(file_path):\n",
    "    custom_id_to_content = {}\n",
    "    failed_custom_ids = []  # To store custom_ids for all failed parsing attempts\n",
    "\n",
    "    # Regex patterns to extract required fields\n",
    "    patterns = {\n",
    "        \"user_preferences\": r'\"user_preferences\"\\s*:\\s*\"(.*?)\"',\n",
    "        \"candidate_perception\": r'\"candidate_perception\"\\s*:\\s*\\{(.*?)\\}'\n",
    "    }\n",
    "\n",
    "    def parse_dict_content(content, custom_id, field_name):\n",
    "        \"\"\"\n",
    "        Parse and clean dictionary-like content.\n",
    "        \"\"\"\n",
    "        cleaned_content = clean_json_content(content)\n",
    "        try:\n",
    "            return json.loads(\"{\" + cleaned_content + \"}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"字段内出错 {field_name} (custom_id: {custom_id}): {content}\")\n",
    "            return {}\n",
    "\n",
    "    def is_valid_extraction(content_dict):\n",
    "        \"\"\"\n",
    "        Validate if all required fields are present and non-empty in the content dictionary.\n",
    "        \"\"\"\n",
    "        required_fields = [\"user_preferences\", \"candidate_perception\"]\n",
    "        return all(field in content_dict and content_dict[field] for field in required_fields)\n",
    "\n",
    "    # Open the file and read line by line\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            try:\n",
    "                # Parse the line as JSON\n",
    "                data = json.loads(line)\n",
    "                custom_id = data.get('custom_id')\n",
    "                content = data.get('response', {}).get('body', {}).get('choices', [])[0].get('message', {}).get('content')\n",
    "\n",
    "                if not custom_id:\n",
    "                    print(f\"Line {line_number}: Missing custom_id.\")\n",
    "                    failed_custom_ids.append(None)\n",
    "                    continue\n",
    "\n",
    "                if not content:\n",
    "                    print(f\"Line {line_number}: Missing content for custom_id {custom_id}.\")\n",
    "                    failed_custom_ids.append(custom_id)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Try parsing content as JSON directly\n",
    "                    content_json = json.loads(content)\n",
    "                    if is_valid_extraction(content_json):\n",
    "                        custom_id_to_content[custom_id] = {\n",
    "                            \"user_preferences\": content_json.get(\"user_preferences\", \"\"),\n",
    "                            \"candidate_perception\": content_json.get(\"candidate_perception\", {})\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"一次解析成功了，但是缺字段 {custom_id}.\")\n",
    "                        failed_custom_ids.append(custom_id)\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    # Fall back to regex extraction and cleaning\n",
    "                    extracted_content = {}\n",
    "                    for field, pattern in patterns.items():\n",
    "                        match = re.search(pattern, content, re.DOTALL)\n",
    "                        if match:\n",
    "                            if field == \"user_preferences\":\n",
    "                                extracted_content[field] = match.group(1)\n",
    "                            else:\n",
    "                                extracted_content[field] = parse_dict_content(match.group(1), custom_id, field)\n",
    "                        else:\n",
    "                            print(f\"正则表达式找不到字段: Missing or invalid {field} for custom_id {custom_id}.\")\n",
    "                            failed_custom_ids.append(custom_id)\n",
    "\n",
    "                    if is_valid_extraction(extracted_content):\n",
    "                        custom_id_to_content[custom_id] = extracted_content\n",
    "                    else:\n",
    "                        print(f\"Line {line_number}: Extracted content invalid for custom_id {custom_id}.\")\n",
    "                        failed_custom_ids.append(custom_id)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Line {line_number}: Error decoding JSON: {e}\")\n",
    "                failed_custom_ids.append(custom_id if 'custom_id' in locals() else None)\n",
    "\n",
    "    # Output all unique failed custom_ids\n",
    "    unique_failed_custom_ids = list(set(failed_custom_ids))\n",
    "    print(\"\\nFailed custom_ids (unique):\")\n",
    "    print(unique_failed_custom_ids)\n",
    "\n",
    "    return custom_id_to_content,unique_failed_custom_ids\n",
    "\n",
    "# Example usage: Replace with actual dataset, phase, and part variables\n",
    "result,unique_failed_custom_ids = process_jsonl_file_with_trailing_comma_fix(f\"{dataset}_random_{phase}_part{part}_result.jsonl\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_failed_ids_from_result(ids, result_file):\n",
    "    \"\"\"\n",
    "    Remove records from the result JSONL file that match the given custom IDs\n",
    "    and overwrite the file with the remaining data.\n",
    "\n",
    "    :param ids: List of custom IDs to remove.\n",
    "    :param result_file: Path to the result JSONL file.\n",
    "    \"\"\"\n",
    "    remaining_records = []\n",
    "\n",
    "    # Read the result file and filter out matching records\n",
    "    with open(result_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                custom_id = data.get('custom_id')\n",
    "                # Keep records that are not in the ids list\n",
    "                if custom_id not in ids:\n",
    "                    remaining_records.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding line: {line.strip()}. Error: {e}\")\n",
    "\n",
    "    # Overwrite the result file with the remaining records\n",
    "    with open(result_file, 'w', encoding='utf-8') as outfile:\n",
    "        for record in remaining_records:\n",
    "            outfile.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Updated {result_file} with {len(remaining_records)} remaining records, removed {len(ids)} records.\")\n",
    "\n",
    "# Example usage\n",
    "failed_custom_ids = unique_failed_custom_ids  # Replace with actual list of failed custom IDs\n",
    "result_file = f\"{dataset}_random_{phase}_part{part}_result.jsonl\"  # Replace with actual result file path\n",
    "remove_failed_ids_from_result(failed_custom_ids, result_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_failed_custom_ids(failed_custom_ids, input_file, output_file):\n",
    "    \"\"\"\n",
    "    Extract records from the input JSONL file that match the failed custom IDs\n",
    "    and write them to a new file.\n",
    "\n",
    "    :param failed_custom_ids: List of custom IDs to extract.\n",
    "    :param input_file: Path to the input JSONL file.\n",
    "    :param output_file: Path to the output JSONL file.\n",
    "    \"\"\"\n",
    "    failed_records = []\n",
    "\n",
    "    # Open the input file and read line by line\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                # Parse the line as JSON\n",
    "                data = json.loads(line)\n",
    "                custom_id = data.get('custom_id')\n",
    "\n",
    "                # Check if the custom_id is in the failed_custom_ids list\n",
    "                if custom_id in failed_custom_ids:\n",
    "                    failed_records.append(data)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding line: {line.strip()}. Error: {e}\")\n",
    "\n",
    "    # Write the extracted records to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for record in failed_records:\n",
    "            outfile.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Extracted {len(failed_records)} records to {output_file}.\")\n",
    "\n",
    "# Example usage\n",
    "failed_custom_ids = unique_failed_custom_ids # Replace with actual list of failed custom IDs\n",
    "input_file = f\"{dataset}_random_{phase}_part{part}.jsonl\"  # Replace with actual file path\n",
    "output_file = f\"{dataset}_random_{phase}_part{part}_.jsonl\"  # Replace with desired output file path\n",
    "extract_failed_custom_ids(failed_custom_ids, input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test.jsonl 是一个本地示例文件，purpose必须是batch\n",
    "file_object = client.files.create(file=Path(f\"{dataset}_random_{phase}_part{part}_.jsonl\"), purpose=\"batch\")\n",
    "\n",
    "print(file_object.model_dump_json())\n",
    "print(file_object.id)  # 打印文件id\n",
    "file_object_id=file_object.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = client.batches.create(\n",
    "    input_file_id=file_object_id,  # 上传文件返回的 id\n",
    "    endpoint=\"/v1/chat/completions\",  # 大语言模型固定填写，/v1/chat/completions\n",
    "    completion_window=\"24h\"  # 当前只支持24h，24小时未运行完会超时\n",
    ")\n",
    "print(batch)\n",
    "print(batch.id)  # 打印Batch任务的id\n",
    "batch_id=batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查询\n",
    "batch = client.batches.retrieve('')  # 将batch_id替换为Batch任务的id\n",
    "print(batch)\n",
    "print(batch.error_file_id)\n",
    "print(batch.output_file_id)  # 打印输出文件id\n",
    "error_file_id=batch.error_file_id\n",
    "output_file_id=batch.output_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = client.files.content(file_id=output_file_id)\n",
    "# 打印结果文件内容\n",
    "print(content.text)\n",
    "# 保存结果文件至本地\n",
    "content.write_to_file(f\"{dataset}_random_{phase}_part{part}_result_.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'{dataset}_{phase}.pkl', 'wb') as file:\n",
    "        pickle.dump(result, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
